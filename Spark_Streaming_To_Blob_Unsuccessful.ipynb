{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XxBZ3csI_UC"
      },
      "outputs": [],
      "source": [
        "event_hub_namespace = \"iesstsabbdbaa-grp-06-10\"\n",
        "topic1_consumer_string = 'Endpoint=sb://iesstsabbadbaa-grp-06-10.servicebus.windows.net/;SharedAccessKeyName=consumer;SharedAccessKey=5RErxrMbiKdB6RKF8OrbXS8MJxM8gwb5D+AEhIB3zAA=;EntityPath=passenger_requests_10'\n",
        "topic2_consumer_string = 'Endpoint=sb://iesstsabbadbaa-grp-06-10.servicebus.windows.net/;SharedAccessKeyName=consumer;SharedAccessKey=AP+NyIt9D0Jg/yC9+ATTtW1/03WuNvUB4+AEhAkSlUE=;EntityPath=ride_status_10'\n",
        "topic1_name = \"passenger_requests_10\"\n",
        "topic2_name = \"ride_status_10\"\n",
        "consumer_String = \"Endpoint=sb://iesstsabbadbaa-grp-06-10.servicebus.windows.net/;SharedAccessKeyName=Consumer_10;SharedAccessKey=jPZjSqQc7HIox8LooseofiUY2mg5/pO/J+AEhDJ4b50=\"\n",
        "\n",
        "# Azure Blob Storage\n",
        "account_name = \"iesstsabbdbaa\"\n",
        "account_key = 'ZT6z+TYSxF0Xdm0vOCRbIpWoBss2BxOU0EcP2UDceddHX7Kyi8gyJvjyWG5THNp2HOprCHmblb2f+AStp8mAGw=='\n",
        "container_name = \"group10\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvNBCCikns8n"
      },
      "outputs": [],
      "source": [
        "import os, time, subprocess\n",
        "\n",
        "spark_version = subprocess.run(\n",
        "    \"curl -s https://downloads.apache.org/spark/ | grep -o 'spark-3\\\\.[0-9]\\\\+\\\\.[0-9]\\\\+' | sort -V | tail -1\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ").stdout.strip()\n",
        "\n",
        "spark_release=spark_version\n",
        "hadoop_version='hadoop3'\n",
        "\n",
        "start=time.time()\n",
        "os.environ['SPARK_RELEASE']=spark_release\n",
        "os.environ['HADOOP_VERSION']=hadoop_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_release}-bin-{hadoop_version}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbmgU_XWyksN"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "  # Run below commands in google colab\n",
        "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null # install Java8\n",
        "  !wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # download spark-3.3.X\n",
        "  !tar xf ${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # unzip it\n",
        "\n",
        "  !pip install -q findspark # install findspark\n",
        "  # findspark find your Spark Distribution and sets necessary environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm-UaiQQwGs1"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Check the pyspark version\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "from pyspark.sql.functions import col, from_unixtime, to_timestamp, window, session_window\n",
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJfgAfne-2vH",
        "outputId": "00736166-c020-4156-abad-13f717abe5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.5.5\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaP4vQOa3R0m"
      },
      "outputs": [],
      "source": [
        "jar_dependencies = \",\".join([\n",
        "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5\",  # Updated to 3.5.5\n",
        "    \"org.apache.spark:spark-avro_2.12:3.5.5\",            # Updated to 3.5.5\n",
        "    \"org.apache.hadoop:hadoop-azure:3.3.1\",              # Hadoop Azure connector (unchanged)\n",
        "    \"com.microsoft.azure:azure-storage:8.6.6\"            # Azure Blob SDK (unchanged)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ipVMTUV7w2j3"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StreamingAVROFromKafka\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config(\"spark.jars.packages\", jar_dependencies) \\\n",
        "    .config(f\"fs.azure.account.key.{account_name}.blob.core.windows.net\", account_key) \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dtp6EplJoQh7"
      },
      "outputs": [],
      "source": [
        "kafkaConf1 = {\n",
        "    \"kafka.bootstrap.servers\": f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{topic1_consumer_string}\";',\n",
        "    \"subscribe\": topic1_name,\n",
        "    \"startingOffsets\": \"latest\",\n",
        "    \"enable.auto.commit\": \"true \",\n",
        "    \"groupIdPrefix\": \"Stream_Analytics_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}\n",
        "\n",
        "kafkaConf2 = {\n",
        "    \"kafka.bootstrap.servers\": f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{topic2_consumer_string}\";',\n",
        "    \"subscribe\": topic2_name,\n",
        "    \"startingOffsets\": \"latest\",\n",
        "    \"enable.auto.commit\": \"true \",\n",
        "    \"groupIdPrefix\": \"Stream_Analytics_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DAPsBFWTwEt4"
      },
      "outputs": [],
      "source": [
        "PASSENGER_REQUEST_SCHEMA = \"\"\"\n",
        "{\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"PassengerRequest\",\n",
        "  \"fields\": [\n",
        "    {\"name\": \"request_id\", \"type\": \"string\", \"doc\": \"Unique identifier for the passenger request.\"},\n",
        "    {\"name\": \"user_id\", \"type\": \"string\", \"doc\": \"Unique identifier for the user making the request.\"},\n",
        "    {\"name\": \"event_type\", \"type\": {\"type\": \"enum\", \"name\": \"EventType\", \"symbols\": [\"request\", \"cancel\"]}, \"doc\": \"Type of event, either a ride request or a cancellation.\"},\n",
        "    {\"name\": \"timestamp\", \"type\": \"long\", \"doc\": \"Epoch timestamp indicating when the request was made.\"},\n",
        "    {\"name\": \"pickup_latitude\", \"type\": \"double\", \"doc\": \"Latitude coordinate of the pickup location.\"},\n",
        "    {\"name\": \"pickup_longitude\", \"type\": \"double\", \"doc\": \"Longitude coordinate of the pickup location.\"},\n",
        "    {\"name\": \"pickup_address\", \"type\": \"string\", \"doc\": \"Readable address of the pickup location.\"},\n",
        "    {\"name\": \"dropoff_latitude\", \"type\": \"double\", \"doc\": \"Latitude coordinate of the dropoff location.\"},\n",
        "    {\"name\": \"dropoff_longitude\", \"type\": \"double\", \"doc\": \"Longitude coordinate of the dropoff location.\"},\n",
        "    {\"name\": \"dropoff_address\", \"type\": \"string\", \"doc\": \"Readable address of the dropoff location.\"},\n",
        "    {\"name\": \"payment_method\", \"type\": {\"type\": \"enum\", \"name\": \"PaymentMethod\", \"symbols\": [\"credit_card\", \"debit_card\", \"cash\", \"mobile_payment\"]}, \"doc\": \"Payment method chosen by the user.\"},\n",
        "    {\"name\": \"vehicle_type\", \"type\": {\"type\": \"enum\", \"name\": \"VehicleType\", \"symbols\": [\"standard\", \"premium\", \"luxury\", \"pool\"]}, \"doc\": \"Type of vehicle requested for the ride.\"},\n",
        "    {\"name\": \"num_passengers\", \"type\": \"int\", \"doc\": \"Number of passengers included in the ride request.\"},\n",
        "    {\"name\": \"fare_estimate\", \"type\": \"float\", \"doc\": \"Estimated fare for the requested ride.\"},\n",
        "    {\"name\": \"ride_purpose\", \"type\": {\"type\": \"enum\", \"name\": \"RidePurpose\", \"symbols\": [\"commute\", \"airport\", \"business\", \"leisure\"]}, \"doc\": \"Purpose of the ride request.\"},\n",
        "    {\"name\": \"device_type\", \"type\": {\"type\": \"enum\", \"name\": \"DeviceType\", \"symbols\": [\"mobile\", \"web\"]}, \"doc\": \"Type of device used to make the ride request.\"}]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "RIDE_STATUS_SCHEMA = \"\"\"\n",
        "{\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"RideStatus\",\n",
        "  \"fields\": [\n",
        "    {\"name\": \"ride_id\", \"type\": \"string\", \"doc\": \"Unique identifier for the ride.\"},\n",
        "    {\"name\": \"request_id\", \"type\": \"string\", \"doc\": \"Identifier linking this ride status to the original ride request.\"},\n",
        "    {\"name\": \"status\", \"type\": {\"type\": \"enum\", \"name\": \"RideStatusEnum\", \"symbols\": [\"accepted\", \"ongoing\", \"completed\", \"cancelled\"]}, \"doc\": \"Current status of the ride.\"},\n",
        "    {\"name\": \"timestamp\", \"type\": \"long\", \"doc\": \"Epoch timestamp indicating when the status was recorded.\"},\n",
        "    {\"name\": \"driver_id\", \"type\": \"string\", \"doc\": \"Unique identifier for the driver assigned to the ride.\"},\n",
        "    {\"name\": \"estimated_arrival_time\", \"type\": \"long\", \"doc\": \"Estimated epoch timestamp of when the driver is expected to arrive at the pickup location.\"},\n",
        "    {\"name\": \"actual_arrival_time\", \"type\": [\"null\", \"long\"], \"default\": \"null\", \"doc\": \"Actual epoch timestamp when the driver arrived at the pickup location. Null if not yet arrived.\"},\n",
        "    {\"name\": \"ride_duration\", \"type\": [\"null\", \"int\"], \"default\": \"null\", \"doc\": \"Duration of the ride in minutes. Null if ride is not completed.\"},\n",
        "    {\"name\": \"distance_traveled\", \"type\": [\"null\", \"float\"], \"default\": \"null\", \"doc\": \"Total distance traveled during the ride in kilometers. Null if ride is not completed.\"},\n",
        "    {\"name\": \"fare\", \"type\": \"float\", \"doc\": \"Final fare amount for the ride.\"},\n",
        "    {\"name\": \"surge_multiplier\", \"type\": \"float\", \"doc\": \"Surge pricing multiplier applied to the fare.\"},\n",
        "    {\"name\": \"traffic_condition\", \"type\": {\"type\": \"enum\", \"name\": \"TrafficCondition\", \"symbols\": [\"low\", \"medium\", \"high\"]}, \"doc\": \"Traffic conditions during the ride.\"},\n",
        "    {\"name\": \"weather_condition\", \"type\": {\"type\": \"enum\", \"name\": \"WeatherCondition\", \"symbols\": [\"clear\", \"rainy\", \"snowy\", \"foggy\", \"stormy\"]}, \"doc\": \"Weather conditions at the time of the ride.\"},\n",
        "    {\"name\": \"day_of_week\", \"type\": {\"type\": \"enum\", \"name\": \"DayOfWeek\", \"symbols\": [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]}, \"doc\": \"Day of the week when the ride took place.\"},\n",
        "    {\"name\": \"driver_rating\", \"type\": [\"null\", \"float\"], \"default\": \"null\", \"doc\": \"Driver's rating for the ride (scale 1-5). Null if ride not completed.\"},\n",
        "    {\"name\": \"cancellation_reason\", \"type\": [\"null\", {\"type\": \"enum\", \"name\": \"CancellationReason\", \"symbols\": [\"user_request\", \"driver_unavailable\", \"technical_issue\"]}], \"default\": \"null\", \"doc\": \"Reason for ride cancellation, if applicable. Null if not cancelled.\"},\n",
        "    {\"name\": \"payment_status\", \"type\": {\"type\": \"enum\", \"name\": \"PaymentStatus\", \"symbols\": [\"pending\", \"completed\", \"failed\"]}, \"doc\": \"Current status of the ride payment.\"}]\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6LuO4bHomsN"
      },
      "outputs": [],
      "source": [
        "# Read from Event Hub using Kafka for topic1\n",
        "df1 = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf1)\n",
        "\n",
        "# Read from Event Hub using Kafka for topic2\n",
        "df2 = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktWJS4TqpH-_"
      },
      "outputs": [],
      "source": [
        "# Deserialize the AVRO messages from the value column\n",
        "df1 = df1.load()\n",
        "df1 = df1.select(from_avro(df1.value, PASSENGER_REQUEST_SCHEMA).alias(\"passenger_request\"))\n",
        "\n",
        "# Deserialize the AVRO messages from the value column\n",
        "df2 = df2.load()\n",
        "df2 = df2.select(from_avro(df2.value, RIDE_STATUS_SCHEMA).alias(\"ride_status\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XB6nWDvpP1x"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df1 = df1.select(\n",
        "    col(\"passenger_request.request_id\"),\n",
        "    col(\"passenger_request.user_id\"),\n",
        "    col(\"passenger_request.event_type\"),\n",
        "    col(\"passenger_request.timestamp\"),\n",
        "    col(\"passenger_request.pickup_latitude\"),\n",
        "    col(\"passenger_request.pickup_longitude\"),\n",
        "    col(\"passenger_request.pickup_address\"),\n",
        "    col(\"passenger_request.dropoff_latitude\"),\n",
        "    col(\"passenger_request.dropoff_longitude\"),\n",
        "    col(\"passenger_request.dropoff_address\"),\n",
        "    col(\"passenger_request.payment_method\"),\n",
        "    col(\"passenger_request.vehicle_type\"),\n",
        "    col(\"passenger_request.num_passengers\"),\n",
        "    col(\"passenger_request.fare_estimate\"),\n",
        "    col(\"passenger_request.ride_purpose\"),\n",
        "    col(\"passenger_request.device_type\")\n",
        ")\n",
        "\n",
        "df2 = df2.select(\n",
        "    col(\"ride_status.ride_id\"),\n",
        "    col(\"ride_status.request_id\"),\n",
        "    col(\"ride_status.status\"),\n",
        "    col(\"ride_status.timestamp\"),\n",
        "    col(\"ride_status.driver_id\"),\n",
        "    col(\"ride_status.estimated_arrival_time\"),\n",
        "    col(\"ride_status.actual_arrival_time\"),\n",
        "    col(\"ride_status.ride_duration\"),\n",
        "    col(\"ride_status.distance_traveled\"),\n",
        "    col(\"ride_status.fare\"),\n",
        "    col(\"ride_status.surge_multiplier\"),\n",
        "    col(\"ride_status.traffic_condition\"),\n",
        "    col(\"ride_status.weather_condition\"),\n",
        "    col(\"ride_status.day_of_week\"),\n",
        "    col(\"ride_status.driver_rating\"),\n",
        "    col(\"ride_status.cancellation_reason\"),\n",
        "    col(\"ride_status.payment_status\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emkq-Rm_p5Qp",
        "outputId": "7a09c131-cdcf-4c33-b33f-bb9cf4af69b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- request_id: string (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- timestamp: long (nullable = true)\n",
            " |-- pickup_latitude: double (nullable = true)\n",
            " |-- pickup_longitude: double (nullable = true)\n",
            " |-- pickup_address: string (nullable = true)\n",
            " |-- dropoff_latitude: double (nullable = true)\n",
            " |-- dropoff_longitude: double (nullable = true)\n",
            " |-- dropoff_address: string (nullable = true)\n",
            " |-- payment_method: string (nullable = true)\n",
            " |-- vehicle_type: string (nullable = true)\n",
            " |-- num_passengers: integer (nullable = true)\n",
            " |-- fare_estimate: float (nullable = true)\n",
            " |-- ride_purpose: string (nullable = true)\n",
            " |-- device_type: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe-6EjmEp8az",
        "outputId": "905e9d09-35f0-4eb1-abdd-5739ae550127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ride_id: string (nullable = true)\n",
            " |-- request_id: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- timestamp: long (nullable = true)\n",
            " |-- driver_id: string (nullable = true)\n",
            " |-- estimated_arrival_time: long (nullable = true)\n",
            " |-- actual_arrival_time: long (nullable = true)\n",
            " |-- ride_duration: integer (nullable = true)\n",
            " |-- distance_traveled: float (nullable = true)\n",
            " |-- fare: float (nullable = true)\n",
            " |-- surge_multiplier: float (nullable = true)\n",
            " |-- traffic_condition: string (nullable = true)\n",
            " |-- weather_condition: string (nullable = true)\n",
            " |-- day_of_week: string (nullable = true)\n",
            " |-- driver_rating: float (nullable = true)\n",
            " |-- cancellation_reason: string (nullable = true)\n",
            " |-- payment_status: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSNlkyVepa0H",
        "outputId": "375218e2-018d-4d46-9c2b-7e5d1bf8f6e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('wasbs://group10@iesstsabbdbaa.blob.core.windows.net/stream-output/',\n",
              " 'wasbs://group10@iesstsabbdbaa.blob.core.windows.net/checkpoint/')"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_path = f\"wasbs://{container_name}@{account_name}.blob.core.windows.net/stream-output/\"\n",
        "checkpoint_path = f\"wasbs://{container_name}@{account_name}.blob.core.windows.net/checkpoint/\"\n",
        "\n",
        "output_path, checkpoint_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mSnvxykcQER4",
        "outputId": "0f624e62-2395-4a36-88f5-c9a576b9e823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure-identity\n",
            "  Downloading azure_identity-1.21.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.25.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting azure-core>=1.31.0 (from azure-identity)\n",
            "  Downloading azure_core-1.33.0-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure-identity) (43.0.3)\n",
            "Collecting msal>=1.30.0 (from azure-identity)\n",
            "  Downloading msal-1.32.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity)\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity) (4.13.2)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure-identity) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure-identity) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure-identity) (1.17.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity) (2.10.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2025.1.31)\n",
            "Downloading azure_identity-1.21.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_storage_blob-12.25.1-py3-none-any.whl (406 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.0/407.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.33.0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.1/207.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading msal-1.32.0-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: isodate, azure-core, azure-storage-blob, msal, msal-extensions, azure-identity\n",
            "Successfully installed azure-core-1.33.0 azure-identity-1.21.0 azure-storage-blob-12.25.1 isodate-0.7.2 msal-1.32.0 msal-extensions-1.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install azure-identity azure-storage-blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "ihhXIKJCVKKB",
        "outputId": "f6958153-c535-4a97-f5ee-f6a0c69af1b8"
      },
      "outputs": [
        {
          "ename": "StreamingQueryException",
          "evalue": "[STREAM_FAILED] Query [id = ade37c7d-e0d3-4736-914e-ccefd125d7cd, runId = e093fe4e-da15-4d27-ab7a-4cbeaa32d7a4] terminated with exception: Failed to create new KafkaAdminClient",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-04919e7b4b85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.5.5-bin-hadoop3/python/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.5-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = ade37c7d-e0d3-4736-914e-ccefd125d7cd, runId = e093fe4e-da15-4d27-ab7a-4cbeaa32d7a4] terminated with exception: Failed to create new KafkaAdminClient"
          ]
        }
      ],
      "source": [
        "df1.writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .start() \\\n",
        "    .awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "GjwWf5TopcHO",
        "outputId": "b8462c49-5290-4998-d094-590251213aa5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-d172cf735f85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"passenger_query\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"5 seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"passenger_query\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.5-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "query_passenger = df1.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"checkpointLocation\", f\"{checkpoint_path}/passenger\") \\\n",
        "    .option(\"path\", f\"{output_path}/passenger\") \\\n",
        "    .queryName(\"passenger_query\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "q = spark.streams.get(\"passenger_query\")\n",
        "print(q.lastProgress)        # estado del último micro-batch\n",
        "print(q.recentProgress)      # lista de batches más recientes\n",
        "\n",
        "query_passenger.awaitTermination()\n",
        "\n",
        "query_ride = df2.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"checkpointLocation\", f\"{checkpoint_path}/ride\") \\\n",
        "    .option(\"path\", f\"{output_path}/ride\") \\\n",
        "    .queryName(\"ride_query\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "query_ride.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQaXFsCsptJ1"
      },
      "outputs": [],
      "source": [
        "# Get the list of active streaming queries\n",
        "active_queries = spark.streams.active\n",
        "\n",
        "# Print details about each active query\n",
        "for query in active_queries:\n",
        "    print(f\"Query Name: {query.name}\")\n",
        "    print(f\"Query ID: {query.id}\")\n",
        "    print(f\"Query Status: {query.status}\")\n",
        "    print(f\"Is Query Active: {query.isActive}\")\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}