{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Streaming to Blob Successful"
      ],
      "metadata": {
        "id": "dUOAscS7F1Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-eventhub fastavro faker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zTiQ39gEwuW",
        "outputId": "605c9b5b-eccf-4312-ffe2-fbe78d51e8d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-eventhub\n",
            "  Downloading azure_eventhub-5.15.0-py3-none-any.whl.metadata (73 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/73.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.1/73.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting faker\n",
            "  Downloading faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting azure-core>=1.27.0 (from azure-eventhub)\n",
            "  Downloading azure_core-1.33.0-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from azure-eventhub) (4.13.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.27.0->azure-eventhub) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.27.0->azure-eventhub) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2025.1.31)\n",
            "Downloading azure_eventhub-5.15.0-py3-none-any.whl (327 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.33.0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.1/207.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastavro, faker, azure-core, azure-eventhub\n",
            "Successfully installed azure-core-1.33.0 azure-eventhub-5.15.0 faker-37.1.0 fastavro-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, subprocess\n",
        "\n",
        "spark_version = subprocess.run(\n",
        "    \"curl -s https://downloads.apache.org/spark/ | grep -o 'spark-3\\\\.[0-9]\\\\+\\\\.[0-9]\\\\+' | sort -V | tail -1\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ").stdout.strip()\n",
        "\n",
        "os.environ['SPARK_RELEASE'] = spark_version\n",
        "os.environ['HADOOP_VERSION'] = 'hadoop3'\n",
        "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ['SPARK_HOME'] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-hadoop3.tgz\n",
        "!tar xf ${SPARK_RELEASE}-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "hovhihAFF63E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "from pyspark.sql.functions import col, from_unixtime, to_timestamp, window, session_window\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "w22_OMzSGPvX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AVRO_Streaming_EventHub\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .config(\"spark.jars.packages\",\n",
        "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
        "            \"org.apache.spark:spark-avro_2.12:3.5.0,\"\n",
        "            \"org.apache.hadoop:hadoop-azure:3.3.1,\"\n",
        "            \"com.microsoft.azure:azure-storage:8.6.6\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "zfhL_GYAH0cl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "TJnviJRTegi4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "jar_dependencies = \",\".join([\n",
        "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n",
        "    \"org.apache.spark:spark-avro_2.12:3.5.0\",\n",
        "    \"org.apache.hadoop:hadoop-azure:3.3.1\",\n",
        "    \"com.microsoft.azure:azure-storage:8.6.6\"\n",
        "])\n",
        "\n",
        "account_name = \"iesstsabbadbaa\"\n",
        "account_key = \"ZT6z+TYSxF0Xdm0vOCRbIpWoBss2BxOU0EcP2UDceddHX7Kyi8gyJvjyWG5THNp2HOprCHmblb2f+AStp8mAGw==\"\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamingAVROFromKafka\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config(\"spark.jars.packages\", jar_dependencies) \\\n",
        "    .config(f\"fs.azure.account.key.{account_name}.blob.core.windows.net\", account_key) \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "-NXA9JGPeocB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.iesstsabbadbaa.blob.core.windows.net\",\n",
        "    \"ZT6z+TYSxF0Xdm0vOCRbIpWoBss2BxOU0EcP2UDceddHX7Kyi8gyJvjyWG5THNp2HOprCHmblb2f+AStp8mAGw==\"\n",
        ")\n",
        "\n",
        "# Event Hub (Kafka API) Settings\n",
        "eventhub_namespace = 'iesstsabbadbaa-grp-06-10'\n",
        "topic1_name = 'passenger_requests_10'\n",
        "topic2_name = 'ride_status_10'\n",
        "consumer_connection_string = \"Endpoint=sb://iesstsabbadbaa-grp-06-10.servicebus.windows.net/;SharedAccessKeyName=Consumer_10;SharedAccessKey=jPZjSqQc7HIox8LooseofiUY2mg5/pO/J+AEhDJ4b50=\"\n",
        "\n",
        "kafkaConf = {\n",
        "    \"kafka.bootstrap.servers\": f\"{eventhub_namespace}.servicebus.windows.net:9093\",\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{consumer_connection_string.split(\"SharedAccessKey=\")[-1].split(\";\")[0]}\";',\n",
        "    \"startingOffsets\": \"latest\",\n",
        "    \"enable.auto.commit\": \"true\",\n",
        "    \"groupIdPrefix\": \"Stream_Analytics_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}\n",
        "\n",
        "kafkaConf_topic1 = kafkaConf.copy()\n",
        "kafkaConf_topic1[\"subscribe\"] = topic1_name\n",
        "\n",
        "kafkaConf_topic2 = kafkaConf.copy()\n",
        "kafkaConf_topic2[\"subscribe\"] = topic2_name\n"
      ],
      "metadata": {
        "id": "YFYYXpFKIGxZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "passenger_requests_schema = \"\"\"\n",
        "{\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"PassengerRequest\",\n",
        "  \"fields\": [\n",
        "    {\"name\": \"request_id\", \"type\": \"string\"},\n",
        "    {\"name\": \"user_id\", \"type\": \"string\"},\n",
        "    {\"name\": \"event_type\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"EventType\",\n",
        "      \"symbols\": [\"request\", \"cancel\"]\n",
        "    }},\n",
        "    {\"name\": \"timestamp\", \"type\": \"long\"},\n",
        "    {\"name\": \"pickup_latitude\", \"type\": \"double\"},\n",
        "    {\"name\": \"pickup_longitude\", \"type\": \"double\"},\n",
        "    {\"name\": \"pickup_address\", \"type\": \"string\"},\n",
        "    {\"name\": \"dropoff_latitude\", \"type\": \"double\"},\n",
        "    {\"name\": \"dropoff_longitude\", \"type\": \"double\"},\n",
        "    {\"name\": \"dropoff_address\", \"type\": \"string\"},\n",
        "    {\"name\": \"payment_method\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"PaymentMethod\",\n",
        "      \"symbols\": [\"credit_card\", \"debit_card\", \"cash\", \"mobile_payment\"]\n",
        "    }},\n",
        "    {\"name\": \"vehicle_type\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"VehicleType\",\n",
        "      \"symbols\": [\"standard\", \"premium\", \"luxury\", \"pool\"]\n",
        "    }},\n",
        "    {\"name\": \"num_passengers\", \"type\": \"int\"},\n",
        "    {\"name\": \"fare_estimate\", \"type\": \"float\"},\n",
        "    {\"name\": \"ride_purpose\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"RidePurpose\",\n",
        "      \"symbols\": [\"commute\", \"airport\", \"business\", \"leisure\"]\n",
        "    }},\n",
        "    {\"name\": \"device_type\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"DeviceType\",\n",
        "      \"symbols\": [\"mobile\", \"web\"]\n",
        "    }}\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "ride_status_schema = \"\"\"\n",
        "{\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"RideStatus\",\n",
        "  \"fields\": [\n",
        "    {\"name\": \"ride_id\", \"type\": \"string\"},\n",
        "    {\"name\": \"request_id\", \"type\": \"string\"},\n",
        "    {\"name\": \"status\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"RideStatusEnum\",\n",
        "      \"symbols\": [\"accepted\", \"ongoing\", \"completed\", \"cancelled\"]\n",
        "    }},\n",
        "    {\"name\": \"timestamp\", \"type\": \"long\"},\n",
        "    {\"name\": \"driver_id\", \"type\": \"string\"},\n",
        "    {\"name\": \"estimated_arrival_time\", \"type\": \"long\"},\n",
        "    {\"name\": \"actual_arrival_time\", \"type\": [\"null\", \"long\"], \"default\": null},\n",
        "    {\"name\": \"ride_duration\", \"type\": [\"null\", \"int\"], \"default\": null},\n",
        "    {\"name\": \"distance_traveled\", \"type\": [\"null\", \"float\"], \"default\": null},\n",
        "    {\"name\": \"fare\", \"type\": \"float\"},\n",
        "    {\"name\": \"surge_multiplier\", \"type\": \"float\"},\n",
        "    {\"name\": \"traffic_condition\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"TrafficCondition\",\n",
        "      \"symbols\": [\"low\", \"medium\", \"high\"]\n",
        "    }},\n",
        "    {\"name\": \"weather_condition\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"WeatherCondition\",\n",
        "      \"symbols\": [\"clear\", \"rainy\", \"snowy\", \"foggy\", \"stormy\"]\n",
        "    }},\n",
        "    {\"name\": \"day_of_week\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"DayOfWeek\",\n",
        "      \"symbols\": [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "    }},\n",
        "    {\"name\": \"driver_rating\", \"type\": [\"null\", \"float\"], \"default\": null},\n",
        "    {\"name\": \"cancellation_reason\", \"type\": [\"null\", {\n",
        "      \"type\": \"enum\", \"name\": \"CancellationReason\",\n",
        "      \"symbols\": [\"user_request\", \"driver_unavailable\", \"technical_issue\"]\n",
        "    }], \"default\": null},\n",
        "    {\"name\": \"payment_status\", \"type\": {\n",
        "      \"type\": \"enum\", \"name\": \"PaymentStatus\",\n",
        "      \"symbols\": [\"pending\", \"completed\", \"failed\"]\n",
        "    }}\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "passenger_schema_str = json.dumps(json.loads(passenger_requests_schema))\n",
        "rider_schema_str = json.dumps(json.loads(ride_status_schema))\n",
        "\n"
      ],
      "metadata": {
        "id": "GGoXp8ckIbrO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_passenger = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf) \\\n",
        "    .option(\"subscribe\", topic1_name) \\\n",
        "    .load()\n",
        "\n",
        "df_passenger = df_passenger.select(from_avro(df_passenger.value, passenger_schema_str).alias(\"passenger\"))\n",
        "\n",
        "df_passenger = df_passenger.selectExpr(\n",
        "    \"passenger.request_id\",\n",
        "    \"passenger.user_id\",\n",
        "    \"passenger.timestamp\",\n",
        "    \"passenger.pickup_latitude as pickup_lat\",\n",
        "    \"passenger.pickup_longitude as pickup_lon\",\n",
        "    \"passenger.dropoff_latitude as dropoff_lat\",\n",
        "    \"passenger.dropoff_longitude as dropoff_lon\",\n",
        "    \"passenger.event_type\",\n",
        "    \"passenger.vehicle_type\",\n",
        "    \"passenger.fare_estimate\",\n",
        "    \"passenger.num_passengers\",\n",
        "    \"passenger.ride_purpose\",\n",
        "    \"passenger.device_type\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "4mPE22CCC1cs",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_test_passenger = df_passenger.writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"truncate\", False) \\\n",
        "    .start()\n",
        "\n",
        "import time\n",
        "time.sleep(15)\n",
        "\n",
        "query_test_passenger.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "iVnaK5H6zPaB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rider = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf) \\\n",
        "    .option(\"subscribe\", topic2_name) \\\n",
        "    .load()\n",
        "\n",
        "df_rider = df_rider.select(\n",
        "    from_avro(df_rider.value, rider_schema_str).alias(\"rider\")\n",
        ")\n",
        "\n",
        "df_rider = df_rider.selectExpr(\n",
        "    \"rider.ride_id\", \"rider.request_id\", \"rider.status\", \"rider.timestamp\",\n",
        "    \"rider.driver_id\", \"rider.estimated_arrival_time\", \"rider.actual_arrival_time\",\n",
        "    \"rider.ride_duration\", \"rider.distance_traveled\", \"rider.fare\",\n",
        "    \"rider.surge_multiplier\", \"rider.traffic_condition\", \"rider.weather_condition\",\n",
        "    \"rider.day_of_week\", \"rider.driver_rating\", \"rider.cancellation_reason\", \"rider.payment_status\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "StWc7ChZGoVo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start passenger stream to Azure Blob\n",
        "passenger_request_query = df_passenger.writeStream.format(\"parquet\") \\\n",
        "    .option(\"path\", \"wasbs://group10@iesstsabbadbaa.blob.core.windows.net/passenger_request_data\") \\\n",
        "    .option(\"checkpointLocation\", \"wasbs://group10@iesstsabbadbaa.blob.core.windows.net/checkpoints/passenger_request_data\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "# Start rider stream to Azure Blob\n",
        "ride_status_query = df_rider.writeStream.format(\"parquet\") \\\n",
        "    .option(\"path\", \"wasbs://group10@iesstsabbadbaa.blob.core.windows.net/ride_status_data\") \\\n",
        "    .option(\"checkpointLocation\", \"wasbs://group10@iesstsabbadbaa.blob.core.windows.net/checkpoints/ride_status_data\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "# Let both run for a set duration\n",
        "import time\n",
        "time.sleep(500)\n",
        "\n",
        "# Stop streaming queries gracefully\n",
        "print(\"ðŸ›‘ Stopping active queries...\")\n",
        "for query in spark.streams.active:\n",
        "    query.stop()\n",
        "\n",
        "# Shutdown Spark session\n",
        "spark.stop()\n",
        "print(\"âœ… All done! Data should now be in Azure Blob Storage.\")\n"
      ],
      "metadata": {
        "id": "mzYqR-s86ar4",
        "outputId": "3c77a939-d683-46d9-981a-04ab45b00a90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ›‘ Stopping active queries...\n",
            "âœ… All done! Data should now be in Azure Blob Storage.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
